---
title: "Time Series Analysis Spring 2023"
subtitle: "10. VECM Models"
author: "Maciej Świtała"
date: "04/05/2023"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE, 
                      cache   = FALSE,
                      message = FALSE, 
                      warning = FALSE)
options(scipen = 10)
```

## 1 Data importing

```{r}
library(tidyverse)
library(xts)
library(vars)
```

We will work today again with the data concerning price indices:

* CPI - Consumer Price Index for All Urban Consumers: Energy, 1982=100, https://fred.stlouisfed.org/series/CPIENGSL
* PPI - Producer Price Index: Finished Energy Goods, 1982=100, https://fred.stlouisfed.org/series/WPSFD4121
* Monthly data: 01.1986-02.2019
* Source: U.S. Department of Labor, Bureau of Labor Statistics
* Economic Data - FRED(R) II - http://research.stlouisfed.org/fred2/

Importing data from the `csv` file:
```{r}
ppi_cpi <- read.csv("C:/Users/Asus/Downloads/TSA_2023_project_data_1.csv")
```

Verification of the structure:
```{r}
ppi_cpi %>% glimpse()
ppi_cpi %>% head()
ppi_cpi %>% tail()
```

It seems to be OK!

Now, we can transform it into the `xts` object:
```{r}
ppi_cpi$date <- as.Date(ppi_cpi[, 1], format = '%d/%m/%Y')
ppi_cpi <- xts(ppi_cpi[, -1], ppi_cpi$date)
```

And finally we can plot variables on the graph:
```{r}
plot(ppi_cpi[, c(4, 8)],
     col = c("black", "blue"),
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "Producer prices and consumer prices",
     legend.loc = "topleft")
```

```{r}
ppi_cpi <- ppi_cpi[, c(4, 8)]


```
## 2 Johansen cointegration test

We have already performed a univariate cointegration test of Engle and Granger. Our conclusions were that `ppi` and `cpi` variables are cointegrated of order ~CI(1,1). 

As an alternative, we will now perform a multivariate test of Johansen. We will assume the `K=4` lag structure and also include seasonal dummy variables. Anyway, this is somewhat arbitrary decision.

```{r}
johan.test.trace <- 
  ca.jo(ppi_cpi, # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "trace",  # type of the test: trace or eigen
        K = 4,           # lag order of the series (levels) in the VAR
        season = 12) 
summary(johan.test.trace) 
```

 Select the optimal lag order using information criteria
```{r}
lag_order <- VARselect(ppi_cpi, lag.max = 10, type = "const")
print(lag_order)
```

Let's find interpretation of the test results:
```{r }
cbind(summary(johan.test.trace)@teststat, summary(johan.test.trace)@cval)
```

Let's recall that if the test statistic is SMALLER than the critical value, we CANNOT reject the null. If the test statistic is LARGER than the critical value, we REJECT the null.

We start with testing the hypothesis that **r = 0** (2nd row). The testing statistic is greater athat the critical value, hence the null is rejected (at 5% level).

Next, we test that **r <= 1**. In this case, the testing statistic is lower that the critical value, hence the null is NOT rejected.

How many cointegration vectors do we have? Exactly one!

What is the cointegrating vector? It is the first eigenvector (the first column):
```{r}
summary(johan.test.trace)@V
```

Weights W: 
```{r}
summary(johan.test.trace)@W
```

This component includes the adjustment coefficients for particular cointegrating relationships in subsequent equations.

Let's apply the alternative variant of the test:

```{r}
johan.test.eigen <- 
  ca.jo(ppi_cpi, # data 
        ecdet = "const", # "none" for no intercept in cointegrating equation, 
        # "const" for constant term in cointegrating equation and 
        # "trend" for trend variable in cointegrating equation
        type = "eigen",  # type of the test: trace or eigen
        K = 6,           # lag order of the series (levels) in the VAR
        ) 
summary(johan.test.eigen) 
```

The conclusions are the same: one cointegrating vector. The variant of the Johansen test does not have impact on the parameters of cointegrating vector.

Now, let's build the VECM model.

## 3 The VECM model

First we need to define the specification of the VECM model. We already did it - for the purpose of Johansen test we stored two specifications: `trace` and `eigen`.

They give the same cointegrating vector, so any of the two can be used to estimate VECM.

The VEC model is estimated by the `cajorls()` function provided by the `urca` package.

```{r}
ppi_cpi.vec4 <- cajorls(johan.test.eigen, # defined specification
                        r = 1) # number of cointegrating vectors
```



The `$rlm` includes the results of the restricted linear model.
```{r}
summary(ppi_cpi.vec4$rlm)
```



We can extract the cointegrating vector in the following way:
```{r}
ppi_cpi.vec4$beta
```

We can reparametrize the VEC model into VAR (here we use the specification object):
```{r}
ppi_cpi.vec4.asVAR <- vec2var(johan.test.eigen, r = 1)
```

Lets see the result:
```{r}
ppi_cpi.vec4.asVAR
```

Based on the reparametrized model, we can calculate and plot Impulse Response Functions:

```{r}
plot(irf(ppi_cpi.vec4.asVAR, n.ahead = 30))
```

We can also perform variance decomposition:
```{r}
plot(fevd(ppi_cpi.vec4.asVAR, n.ahead = 36))
```

The results are pretty similar to the earlier `VAR(4)` model.

Let's also check if model residuals are autocorrelated.

Residuals can be extracted only from the VAR reparametrized model.

```{r}
head(residuals(ppi_cpi.vec4.asVAR))
serial.test(ppi_cpi.vec4.asVAR)
```

The null is rejected.

You can see the ACF and PACF functions by plotting the results of the `serial.test()`
```{r fig.width=12, fig.height = 12}
plot(serial.test(ppi_cpi.vec4.asVAR))
```

The term EDF stands for Empirical Distribution Function. 

Obviously, for the better quality of the histogram we can extract residuals from the `ppi_cpi.vec4.asVAR` object and create it on our own and visually access dissimilarity to normal distribution.

Let's then create both histograms

```{r}
ppi_cpi.vec4.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of ppi`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(ppi_cpi.vec4.asVAR)[, 1]), 
                            sd = sd(residuals(ppi_cpi.vec4.asVAR)[, 1]))) +
  theme_bw() + 
  labs(
    title = "Density of PPI residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

```{r}
ppi_cpi.vec4.asVAR %>%
  residuals() %>%
  as_tibble() %>%
  ggplot(aes(`resids of cpi`)) +
  geom_histogram(aes(y =..density..),
                 colour = "black", 
                 fill = "pink") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(residuals(ppi_cpi.vec4.asVAR)[, 2]), 
                            sd = sd(residuals(ppi_cpi.vec4.asVAR)[, 2]))) +
  theme_bw() + 
  labs(
    title = "Density of CPI residuals", 
    y = "", x = "",
    caption = "source: own calculations"
  )
```

Lets also check normality of residuals more formally. We can apply the Jarque-Bera (JB) test. We have to remember that the Johansen test is sensitive to lack of normality.
```{r}
normality.test(ppi_cpi.vec4.asVAR)
```

The null about normality is rejected.

## 4 Forecasting based on the VECM 

Now we will calculate 14-month-ahead forecasts for both `ppi` and `cpi` variables.

Lets estimate a model on a shorter sample:

```{r}
tail(ppi_cpi, 20)
```

Lets put aside last 14 observations:
```{r}
ppi_cpi.short <- ppi_cpi[1:970, ]
tail(ppi_cpi.short)
```

Now, we now perform again the multivariate test of Johansen, this time on the shorter sample.
```{r}
johan.test.eigen2.short <- ca.jo(ppi_cpi.short,
                                 ecdet = "const",
                                 type = "trace", 
                                 K = 6,
                                 )
summary(johan.test.eigen2.short)
```

We are now ready to run a forecast. First, we need to reparametrize VEC into VAR. As we know, it can be done via the `vec2var()` function.
```{r}
ppi_cpi.vec4.fore <- 
  predict(
    vec2var(
      johan.test.eigen2.short, 
      r = 1),     # no of cointegrating vectors 
    n.ahead = 30, # forecast horizon
    ci = 0.95)    # confidence level for intervals
```

The resulting object has the same structure as forecasts for VAR model.

VEC forecasts for `ppi`:
```{r}
ppi_cpi.vec4.fore$fcst$ppi
```

VEC forecasts for `cpi`:
```{r}
ppi_cpi.vec4.fore$fcst$cpi
```

Lets store it as an `xts` object. The correct set of dates (index) can be extracted from the original `xts` data object.

```{r}
tail(index(ppi_cpi), 30)
ppi_forecast <- xts(ppi_cpi.vec4.fore$fcst$ppi[,-4], 
                    # we exclude the last column with CI
                    tail(index(ppi_cpi), 30))
```

Correction of the variable names:
```{r}
names(ppi_forecast) <- c("PPI_fore", "PPI_lower", "PPI_upper")
```

Lets do the same for `cpi` forecasts:
```{r}
cpi_forecast <- xts(ppi_cpi.vec4.fore$fcst$cpi[, -4],
                    # we exclude the last column with CI
                    tail(index(ppi_cpi), 14))
names(cpi_forecast) <- c("CPI_fore", "CPI_lower", "CPI_upper")
```

Now, we can merge the data together:
```{r}
ppi_cpi <- merge(ppi_cpi, 
                 ppi_forecast,
                 cpi_forecast)
```

Lets compare the forecasted and real data on the plot.

```{r}
plot(ppi_cpi["2017/", c("ppi", "PPI_fore",
                        "PPI_lower", "PPI_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "14 month forecast of energy ppi",
     col = c("black", "blue", "red", "red"))
plot(ppi_cpi["2017/", c("cpi", "CPI_fore",
                        "CPI_lower", "CPI_upper")], 
     major.ticks = "years", 
     grid.ticks.on = "years",
     grid.ticks.lty = 3,
     main = "14 month forecast of energy cpi",
     col = c("black", "blue", "red", "red"))
```


## Exercises 10 

### Exercise 10.1
Calculate forecast accuracy measures (MAE, MSE, MAPE, AMAPE) for above VEC model and compare it with the accuracy of the forecast based on VAR (see labs09).

### Exercise 10.2
Please perform similar analyses for variables `r3m` and `r10y` from the data in the `mrates.csv` file.

Data:

* `r3m`: 3-Month Treasury Constant Maturity Rate,
* `r10y`: 10-Year Treasury Constant Maturity Rate.
* Monthly data: 01.1986-03.2019
* Source: Board of Governors of the Federal Reserve System
* Economic Data - FRED(R) II - http://research.stlouisfed.org/fred2/ 

Import the data from the file `mrates.csv`, convert it into the `xts` object. Apply the Johansen procedure to test for cointegration between the series assuming the model structure based on your results from VAR estimation (nr of lags). Interpret the results.

### Exercise 10.3
Estimate the VEC model. Does the adjustment mechanism work here?

### Exercise 10.4
Create a shorter sample data without last 14 observations and reestimate the above found VEC model on it. Based on the estimated VEC model produce a forecast for next 15 months and compare forecast results with real data (using MAE, MSE, MAPE and AMAPE). Compare forecast results with previous VAR forecasts results.





